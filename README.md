# RAG-Based Document Question Answering System

A Retrieval-Augmented Generation (RAG) based document assistant that allows users to upload PDF files and ask questions about their content. The system uses embeddings, vector search (FAISS), and a locally hosted LLM (Ollama) to generate accurate, context-aware responses in real time with token streaming.

---

## ğŸš€ Features

- PDF document upload
- Text chunking and preprocessing
- Embedding generation
- FAISS-based vector similarity search
- Retrieval-Augmented Generation (RAG)
- Local LLM integration using Ollama
- Real-time token streaming
- FastAPI backend
- Single-page web UI
- Chat-style interface
- Dark mode UI
- Automatic scrolling
- End-to-end pipeline: Upload â†’ Index â†’ Retrieve â†’ Generate

---

## ğŸ›  Tech Stack

### Backend
- Python
- FastAPI
- LangChain
- FAISS
- Ollama (local LLM)
- HuggingFace Embeddings

### Frontend
- HTML
- CSS
- Vanilla JavaScript

### Machine Learning / NLP
- Embeddings
- Vector similarity search
- Chunking strategies
- Retrieval-Augmented Generation (RAG)

---

## ğŸ§  System Architecture

User
â†“
Web UI
â†“
FastAPI Backend
â†“
PDF Ingestion â†’ Chunking â†’ Embeddings â†’ FAISS Index
â†“
User Query â†’ Similarity Search â†’ Context Retrieval
â†“
Prompt Construction
â†“
Local LLM (Ollama)
â†“
Streaming Response to UI

yaml
Copy code

---

## âš™ï¸ How It Works

1. The user uploads a PDF document.
2. The document is split into smaller chunks.
3. Each chunk is converted into vector embeddings.
4. The embeddings are stored in a FAISS vector database.
5. When a user asks a question:
   - The query is embedded.
   - Relevant chunks are retrieved using vector similarity search.
   - These chunks are injected into the LLM prompt.
   - The LLM generates a context-aware response.
6. The response is streamed token-by-token to the UI.

---

## ğŸ“‚ Folder Structure

rag-document-assistant/
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ app.py
â”‚ â”œâ”€â”€ ingest.py
â”‚ â”œâ”€â”€ retrieve.py
â”‚ â”œâ”€â”€ generate.py
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ docs/
â”‚
â”œâ”€â”€ vector_store/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

yaml
Copy code

---

## ğŸ§ª Installation

Clone the repository:

```bash
git clone <your-repo-url>
cd rag-document-assistant
Create and activate a virtual environment:

bash
Copy code
python -m venv venv
source venv/bin/activate   # Mac/Linux
venv\Scripts\activate      # Windows
Install dependencies:

bash
Copy code
pip install -r requirements.txt
â–¶ï¸ Running the Application
1. Start Ollama
bash
Copy code
ollama serve
Pull a model:

bash
Copy code
ollama pull llama2
2. Start the FastAPI Server
bash
Copy code
uvicorn src.app:app --reload
3. Open in Browser
cpp
Copy code
http://127.0.0.1:8000
ğŸ¯ Use Case
This system allows users to:

Upload large PDF documents

Ask natural language questions

Receive accurate, context-aware answers

Get responses in real time via token streaming

Interact with a chat-style UI

ğŸ”® Future Improvements
Source citation display

Chat memory

Markdown rendering

Multi-document support

Drag-and-drop upload

Authentication

Cloud deployment

ğŸ’¡ Why This Project Matters
This project demonstrates:

End-to-end RAG pipeline implementation

Working with vector databases

Local LLM deployment

Backend API design

Real-time streaming systems

Practical AI system design

Debugging real-world ML pipelines